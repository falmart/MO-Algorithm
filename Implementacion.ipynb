{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639256bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 1: Importaciones y configuración inicial\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.optimize import minimize\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuración de matplotlib para mejores gráficos\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Celda 2: Definición de la clase de optimización\n",
    "class OptimizacionNoConvexa:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Clase para analizar y optimizar la función f(x,y) = ln²(e^x + y² + 1)\n",
    "        \"\"\"\n",
    "        self.funcion_objetivo = self.f\n",
    "        self.gradiente = self.grad_f\n",
    "        \n",
    "    def f(self, params):\n",
    "        \"\"\"\n",
    "        Función objetivo: f(x,y) = ln²(e^x + y² + 1)\n",
    "        \"\"\"\n",
    "        x, y = params\n",
    "        inner = np.exp(x) + y**2 + 1\n",
    "        return (np.log(inner))**2\n",
    "    \n",
    "    def grad_f(self, params):\n",
    "        \"\"\"\n",
    "        Gradiente de la función objetivo\n",
    "        \"\"\"\n",
    "        x, y = params\n",
    "        inner = np.exp(x) + y**2 + 1\n",
    "        log_inner = np.log(inner)\n",
    "        \n",
    "        df_dx = 2 * log_inner * (np.exp(x) / inner)\n",
    "        df_dy = 2 * log_inner * (2 * y / inner)\n",
    "        \n",
    "        return np.array([df_dx, df_dy])\n",
    "    \n",
    "    def descenso_gradiente(self, punto_inicial, learning_rate=0.1, max_iter=1000, tol=1e-8):\n",
    "        \"\"\"\n",
    "        Algoritmo de Descenso de Gradiente\n",
    "        \"\"\"\n",
    "        punto_actual = np.array(punto_inicial, dtype=float)\n",
    "        trayectoria = [punto_actual.copy()]\n",
    "        valores_funcion = [self.f(punto_actual)]\n",
    "        \n",
    "        for i in range(max_iter):\n",
    "            grad = self.grad_f(punto_actual)\n",
    "            nuevo_punto = punto_actual - learning_rate * grad\n",
    "            \n",
    "            # Verificar límites del dominio [-100, 100]\n",
    "            nuevo_punto = np.clip(nuevo_punto, -100, 100)\n",
    "            \n",
    "            trayectoria.append(nuevo_punto.copy())\n",
    "            valores_funcion.append(self.f(nuevo_punto))\n",
    "            \n",
    "            # Criterio de parada\n",
    "            if np.linalg.norm(nuevo_punto - punto_actual) < tol:\n",
    "                break\n",
    "                \n",
    "            punto_actual = nuevo_punto\n",
    "        \n",
    "        resultado = {\n",
    "            'punto_optimo': nuevo_punto.tolist(),\n",
    "            'valor_optimo': float(self.f(nuevo_punto)),\n",
    "            'iteraciones': len(trayectoria),\n",
    "            'norma_gradiente': float(np.linalg.norm(grad)),\n",
    "            'trayectoria': [p.tolist() for p in trayectoria],\n",
    "            'valores_funcion': valores_funcion,\n",
    "            'learning_rate': learning_rate\n",
    "        }\n",
    "        \n",
    "        return resultado\n",
    "    \n",
    "    def bfgs_optimizacion(self, punto_inicial):\n",
    "        \"\"\"\n",
    "        Algoritmo BFGS usando scipy.optimize\n",
    "        \"\"\"\n",
    "        tiempo_inicio = time.time()\n",
    "        \n",
    "        resultado = minimize(self.funcion_objetivo, punto_inicial, \n",
    "                           method='BFGS', jac=self.gradiente,\n",
    "                           options={'gtol': 1e-8, 'disp': False})\n",
    "        \n",
    "        tiempo_ejecucion = time.time() - tiempo_inicio\n",
    "        \n",
    "        resultado_dict = {\n",
    "            'punto_optimo': resultado.x.tolist(),\n",
    "            'valor_optimo': float(resultado.fun),\n",
    "            'iteraciones': int(resultado.nit),\n",
    "            'norma_gradiente': float(np.linalg.norm(resultado.jac)),\n",
    "            'tiempo_ejecucion': tiempo_ejecucion,\n",
    "            'exito': bool(resultado.success)\n",
    "        }\n",
    "        \n",
    "        return resultado_dict\n",
    "    \n",
    "    def nelder_mead_optimizacion(self, punto_inicial):\n",
    "        \"\"\"\n",
    "        Algoritmo Nelder-Mead usando scipy.optimize\n",
    "        \"\"\"\n",
    "        tiempo_inicio = time.time()\n",
    "        \n",
    "        resultado = minimize(self.funcion_objetivo, punto_inicial, \n",
    "                           method='Nelder-Mead',\n",
    "                           options={'xatol': 1e-8, 'fatol': 1e-8, 'disp': False})\n",
    "        \n",
    "        tiempo_ejecucion = time.time() - tiempo_inicio\n",
    "        \n",
    "        resultado_dict = {\n",
    "            'punto_optimo': resultado.x.tolist(),\n",
    "            'valor_optimo': float(resultado.fun),\n",
    "            'iteraciones': int(resultado.nit),\n",
    "            'tiempo_ejecucion': tiempo_ejecucion,\n",
    "            'exito': bool(resultado.success)\n",
    "        }\n",
    "        \n",
    "        return resultado_dict\n",
    "    \n",
    "    def experimento_completo(self, punto_inicial, learning_rates=[0.01, 0.1, 0.5]):\n",
    "        \"\"\"\n",
    "        Ejecuta todos los algoritmos para un punto inicial dado\n",
    "        \"\"\"\n",
    "        resultados = {\n",
    "            'punto_inicial': punto_inicial,\n",
    "            'descenso_gradiente': {},\n",
    "            'bfgs': {},\n",
    "            'nelder_mead': {}\n",
    "        }\n",
    "        \n",
    "        # Probar diferentes learning rates para Descenso de Gradiente\n",
    "        for lr in learning_rates:\n",
    "            resultados['descenso_gradiente'][f'lr_{lr}'] = self.descenso_gradiente(\n",
    "                punto_inicial, learning_rate=lr\n",
    "            )\n",
    "        \n",
    "        # BFGS\n",
    "        resultados['bfgs'] = self.bfgs_optimizacion(punto_inicial)\n",
    "        \n",
    "        # Nelder-Mead\n",
    "        resultados['nelder_mead'] = self.nelder_mead_optimizacion(punto_inicial)\n",
    "        \n",
    "        return resultados\n",
    "    \n",
    "    def visualizar_funcion(self, rango_x=(-5, 5), rango_y=(-5, 5), puntos=100):\n",
    "        \"\"\"\n",
    "        Visualización 3D de la función objetivo\n",
    "        \"\"\"\n",
    "        x = np.linspace(rango_x[0], rango_x[1], puntos)\n",
    "        y = np.linspace(rango_y[0], rango_y[1], puntos)\n",
    "        X, Y = np.meshgrid(x, y)\n",
    "        \n",
    "        Z = np.zeros_like(X)\n",
    "        for i in range(X.shape[0]):\n",
    "            for j in range(X.shape[1]):\n",
    "                Z[i,j] = self.f([X[i,j], Y[i,j]])\n",
    "        \n",
    "        # Crear figura con subplots\n",
    "        fig = plt.figure(figsize=(18, 6))\n",
    "        \n",
    "        # Subplot 1: Superficie 3D\n",
    "        ax1 = fig.add_subplot(131, projection='3d')\n",
    "        surf = ax1.plot_surface(X, Y, Z, cmap=cm.viridis, alpha=0.8)\n",
    "        ax1.set_title('Función: $ln^2(e^x + y^2 + 1)$')\n",
    "        ax1.set_xlabel('x')\n",
    "        ax1.set_ylabel('y')\n",
    "        ax1.set_zlabel('f(x,y)')\n",
    "        fig.colorbar(surf, ax=ax1, shrink=0.5, aspect=5)\n",
    "        \n",
    "        # Subplot 2: Curvas de nivel\n",
    "        ax2 = fig.add_subplot(132)\n",
    "        contour = ax2.contour(X, Y, Z, levels=20)\n",
    "        ax2.set_title('Curvas de Nivel')\n",
    "        ax2.set_xlabel('x')\n",
    "        ax2.set_ylabel('y')\n",
    "        plt.colorbar(contour, ax=ax2)\n",
    "        \n",
    "        # Subplot 3: Heatmap\n",
    "        ax3 = fig.add_subplot(133)\n",
    "        im = ax3.imshow(Z, extent=[rango_x[0], rango_x[1], rango_y[0], rango_y[1]], \n",
    "                       origin='lower', cmap=cm.viridis, aspect='auto')\n",
    "        ax3.set_title('Mapa de Calor')\n",
    "        ax3.set_xlabel('x')\n",
    "        ax3.set_ylabel('y')\n",
    "        plt.colorbar(im, ax=ax3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return fig\n",
    "\n",
    "# Celda 3: Funciones auxiliares para manejar archivos JSON\n",
    "def cargar_configuracion(archivo_config='puntos_experimentos.json'):\n",
    "    \"\"\"\n",
    "    Carga la configuración de experimentos desde un archivo JSON\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(archivo_config, 'r') as f:\n",
    "            config = json.load(f)\n",
    "        return config\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: No se encontró el archivo {archivo_config}\")\n",
    "        print(\"Por favor, crea un archivo JSON con la siguiente estructura:\")\n",
    "        print(\"\"\"\n",
    "{\n",
    "    \"puntos_iniciales\": [\n",
    "        [0, 0],\n",
    "        [10, 10],\n",
    "        [50, 50],\n",
    "        [-50, -50],\n",
    "        [100, 0],\n",
    "        [-100, 100]\n",
    "    ],\n",
    "    \"learning_rates\": [0.01, 0.1, 0.5],\n",
    "    \"parametros_adicionales\": {\n",
    "        \"tolerancia\": 1e-8,\n",
    "        \"max_iteraciones\": 1000\n",
    "    }\n",
    "}\n",
    "        \"\"\")\n",
    "        return None\n",
    "\n",
    "def guardar_resultados(resultados, archivo_salida='resultados_optimizacion.json'):\n",
    "    \"\"\"\n",
    "    Guarda los resultados en un archivo JSON\n",
    "    \"\"\"\n",
    "    with open(archivo_salida, 'w') as f:\n",
    "        json.dump(resultados, f, indent=2)\n",
    "\n",
    "def crear_archivo_configuracion_ejemplo():\n",
    "    \"\"\"\n",
    "    Crea un archivo de configuración de ejemplo si no existe\n",
    "    \"\"\"\n",
    "    config_ejemplo = {\n",
    "        \"puntos_iniciales\": [\n",
    "            [0, 0],\n",
    "            [10, 10],\n",
    "            [50, 50],\n",
    "            [-50, -50],\n",
    "            [100, 0],\n",
    "            [-100, 100],\n",
    "            [25, -25],\n",
    "            [-75, 75]\n",
    "        ],\n",
    "        \"learning_rates\": [0.01, 0.1, 0.5],\n",
    "        \"parametros_adicionales\": {\n",
    "            \"tolerancia\": 1e-8,\n",
    "            \"max_iteraciones\": 1000\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open('puntos_experimentos.json', 'w') as f:\n",
    "        json.dump(config_ejemplo, f, indent=2)\n",
    "    \n",
    "    print(\"Archivo de configuración de ejemplo creado: puntos_experimentos.json\")\n",
    "\n",
    "# Celda 4: Función principal de experimentos\n",
    "def ejecutar_experimentos_desde_json(archivo_config='puntos_experimentos.json'):\n",
    "    \"\"\"\n",
    "    Función principal que ejecuta todos los experimentos desde un archivo JSON\n",
    "    \"\"\"\n",
    "    # Cargar configuración\n",
    "    config = cargar_configuracion(archivo_config)\n",
    "    if config is None:\n",
    "        # Crear archivo de ejemplo y salir\n",
    "        crear_archivo_configuracion_ejemplo()\n",
    "        return None, None\n",
    "    \n",
    "    puntos_iniciales = config['puntos_iniciales']\n",
    "    learning_rates = config.get('learning_rates', [0.01, 0.1, 0.5])\n",
    "    \n",
    "    print(f\"Configuración cargada desde: {archivo_config}\")\n",
    "    print(f\"Número de puntos iniciales: {len(puntos_iniciales)}\")\n",
    "    print(f\"Learning rates a probar: {learning_rates}\")\n",
    "    \n",
    "    # Inicializar el optimizador\n",
    "    optimizador = OptimizacionNoConvexa()\n",
    "    \n",
    "    # Visualizar la función\n",
    "    print(\"\\nVisualizando la función objetivo...\")\n",
    "    optimizador.visualizar_funcion()\n",
    "    \n",
    "    # Ejecutar experimentos\n",
    "    print(\"\\nEjecutando experimentos...\")\n",
    "    resultados_totales = {\n",
    "        'archivo_configuracion': archivo_config,\n",
    "        'fecha_ejecucion': time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        'configuracion': {\n",
    "            'puntos_iniciales': puntos_iniciales,\n",
    "            'learning_rates': learning_rates,\n",
    "            'funcion_objetivo': 'ln^2(e^x + y^2 + 1)',\n",
    "            'dominio': '[-100, 100] x [-100, 100]'\n",
    "        },\n",
    "        'experimentos': []\n",
    "    }\n",
    "    \n",
    "    for i, punto in enumerate(puntos_iniciales):\n",
    "        print(f\"\\nProcesando punto inicial {i+1}/{len(puntos_iniciales)}: {punto}\")\n",
    "        \n",
    "        resultados_punto = optimizador.experimento_completo(punto, learning_rates)\n",
    "        resultados_totales['experimentos'].append(resultados_punto)\n",
    "        \n",
    "        # Mostrar resultados parciales\n",
    "        mejor_gd = min(resultados_punto['descenso_gradiente'].values(), \n",
    "                      key=lambda x: x['valor_optimo'])\n",
    "        print(f\"  Mejor GD: {mejor_gd['valor_optimo']:.2e} en {mejor_gd['iteraciones']} iteraciones\")\n",
    "        print(f\"  BFGS: {resultados_punto['bfgs']['valor_optimo']:.2e} en {resultados_punto['bfgs']['iteraciones']} iteraciones\")\n",
    "        print(f\"  Nelder-Mead: {resultados_punto['nelder_mead']['valor_optimo']:.2e} en {resultados_punto['nelder_mead']['iteraciones']} iteraciones\")\n",
    "    \n",
    "    # Guardar resultados\n",
    "    archivo_resultados = f\"resultados_optimizacion_{time.strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    guardar_resultados(resultados_totales, archivo_resultados)\n",
    "    print(f\"\\nResultados guardados en: {archivo_resultados}\")\n",
    "    \n",
    "    return resultados_totales, optimizador, archivo_resultados\n",
    "\n",
    "# Celda 5: Ejecutar los experimentos desde JSON\n",
    "print(\"INICIANDO EXPERIMENTOS DE OPTIMIZACIÓN DESDE ARCHIVO JSON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ESPECIFICAR AQUÍ EL NOMBRE DEL ARCHIVO JSON CON LOS PUNTOS\n",
    "archivo_puntos = \"puntos.json\"  # ← CAMBIA ESTE NOMBRE POR TU ARCHIVO\n",
    "\n",
    "resultados_completos, optimizador, archivo_resultados = ejecutar_experimentos_desde_json(archivo_puntos)\n",
    "\n",
    "if resultados_completos is None:\n",
    "    print(\"No se pudieron ejecutar los experimentos. Se creó un archivo de ejemplo.\")\n",
    "else:\n",
    "    # Celda 6: Análisis y visualización de resultados\n",
    "    def analizar_resultados(archivo_resultados):\n",
    "        with open(archivo_resultados, 'r') as f:\n",
    "            resultados = json.load(f)\n",
    "        \n",
    "        # Convertir a DataFrame para análisis\n",
    "        datos = []\n",
    "        for i, exp in enumerate(resultados['experimentos']):\n",
    "            punto = exp['punto_inicial']\n",
    "            \n",
    "            # Mejor Descenso de Gradiente\n",
    "            mejor_gd = min(exp['descenso_gradiente'].values(), \n",
    "                          key=lambda x: x['valor_optimo'])\n",
    "            \n",
    "            datos.append({\n",
    "                'punto_inicial': f\"({punto[0]}, {punto[1]})\",\n",
    "                'x_inicial': punto[0],\n",
    "                'y_inicial': punto[1],\n",
    "                'gd_valor': mejor_gd['valor_optimo'],\n",
    "                'gd_iteraciones': mejor_gd['iteraciones'],\n",
    "                'gd_gradiente': mejor_gd['norma_gradiente'],\n",
    "                'bfgs_valor': exp['bfgs']['valor_optimo'],\n",
    "                'bfgs_iteraciones': exp['bfgs']['iteraciones'],\n",
    "                'bfgs_gradiente': exp['bfgs']['norma_gradiente'],\n",
    "                'bfgs_tiempo': exp['bfgs']['tiempo_ejecucion'],\n",
    "                'nm_valor': exp['nelder_mead']['valor_optimo'],\n",
    "                'nm_iteraciones': exp['nelder_mead']['iteraciones'],\n",
    "                'nm_tiempo': exp['nelder_mead']['tiempo_ejecucion']\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(datos)\n",
    "        return df, resultados\n",
    "\n",
    "    # Celda 7: Cargar resultados y mostrar resumen\n",
    "    print(\"\\nCargando resultados de optimización...\")\n",
    "    df, resultados_completos = analizar_resultados(archivo_resultados)\n",
    "\n",
    "    print(\"Resumen de resultados:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(df.to_string(index=False))\n",
    "\n",
    "    # Celda 8: Análisis estadístico\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ANÁLISIS ESTADÍSTICO\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    print(\"\\nValores óptimos promedio:\")\n",
    "    print(f\"Descenso Gradiente: {df['gd_valor'].mean():.2e} ± {df['gd_valor'].std():.2e}\")\n",
    "    print(f\"BFGS: {df['bfgs_valor'].mean():.2e} ± {df['bfgs_valor'].std():.2e}\")\n",
    "    print(f\"Nelder-Mead: {df['nm_valor'].mean():.2e} ± {df['nm_valor'].std():.2e}\")\n",
    "\n",
    "    print(\"\\nIteraciones promedio:\")\n",
    "    print(f\"Descenso Gradiente: {df['gd_iteraciones'].mean():.1f} ± {df['gd_iteraciones'].std():.1f}\")\n",
    "    print(f\"BFGS: {df['bfgs_iteraciones'].mean():.1f} ± {df['bfgs_iteraciones'].std():.1f}\")\n",
    "    print(f\"Nelder-Mead: {df['nm_iteraciones'].mean():.1f} ± {df['nm_iteraciones'].std():.1f}\")\n",
    "\n",
    "    # Celda 9: Visualización comparativa\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "    # Gráfico 1: Comparación de valores óptimos\n",
    "    algoritmos = ['GD', 'BFGS', 'NM']\n",
    "    valores_promedio = [df['gd_valor'].mean(), df['bfgs_valor'].mean(), df['nm_valor'].mean()]\n",
    "    valores_std = [df['gd_valor'].std(), df['bfgs_valor'].std(), df['nm_valor'].std()]\n",
    "\n",
    "    axes[0,0].bar(algoritmos, valores_promedio, yerr=valores_std, capsize=5, alpha=0.7)\n",
    "    axes[0,0].set_ylabel('Valor de f(x,y) (escala log)')\n",
    "    axes[0,0].set_yscale('log')\n",
    "    axes[0,0].set_title('Comparación de Valores Óptimos Promedio')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Gráfico 2: Comparación de iteraciones\n",
    "    iter_promedio = [df['gd_iteraciones'].mean(), df['bfgs_iteraciones'].mean(), df['nm_iteraciones'].mean()]\n",
    "    iter_std = [df['gd_iteraciones'].std(), df['bfgs_iteraciones'].std(), df['nm_iteraciones'].std()]\n",
    "\n",
    "    axes[0,1].bar(algoritmos, iter_promedio, yerr=iter_std, capsize=5, alpha=0.7, color='orange')\n",
    "    axes[0,1].set_ylabel('Número de Iteraciones')\n",
    "    axes[0,1].set_title('Comparación de Iteraciones Promedio')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Gráfico 3: Tiempos de ejecución (solo BFGS y Nelder-Mead)\n",
    "    tiempos_promedio = [df['bfgs_tiempo'].mean(), df['nm_tiempo'].mean()]\n",
    "    tiempos_std = [df['bfgs_tiempo'].std(), df['nm_tiempo'].std()]\n",
    "\n",
    "    axes[1,0].bar(['BFGS', 'Nelder-Mead'], tiempos_promedio, yerr=tiempos_std, capsize=5, alpha=0.7, color='green')\n",
    "    axes[1,0].set_ylabel('Tiempo de Ejecución (s)')\n",
    "    axes[1,0].set_title('Comparación de Tiempos de Ejecución')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Gráfico 4: Norma del gradiente final\n",
    "    grad_promedio = [df['gd_gradiente'].mean(), df['bfgs_gradiente'].mean()]\n",
    "    grad_std = [df['gd_gradiente'].std(), df['bfgs_gradiente'].std()]\n",
    "\n",
    "    axes[1,1].bar(['GD', 'BFGS'], grad_promedio, yerr=grad_std, capsize=5, alpha=0.7, color='red')\n",
    "    axes[1,1].set_ylabel('Norma del Gradiente (escala log)')\n",
    "    axes[1,1].set_yscale('log')\n",
    "    axes[1,1].set_title('Comparación de Norma del Gradiente Final')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Celda 10: Análisis de sensibilidad del learning rate\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ANÁLISIS DE SENSIBILIDAD - LEARNING RATE\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Tomar el primer punto de los experimentos para el análisis de sensibilidad\n",
    "    punto_prueba = resultados_completos['experimentos'][0]['punto_inicial']\n",
    "    learning_rates = [0.001, 0.01, 0.1, 0.5, 1.0]\n",
    "\n",
    "    resultados_lr = {}\n",
    "    for lr in learning_rates:\n",
    "        resultado = optimizador.descenso_gradiente(punto_prueba, learning_rate=lr)\n",
    "        resultados_lr[lr] = resultado\n",
    "        print(f\"LR = {lr}: valor = {resultado['valor_optimo']:.2e}, iteraciones = {resultado['iteraciones']}\")\n",
    "\n",
    "    # Celda 11: Visualización de trayectorias\n",
    "    def visualizar_trayectorias(optimizador, punto_inicial, resultados):\n",
    "        \"\"\"Visualiza las trayectorias de optimización\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Crear malla para fondo\n",
    "        x = np.linspace(min(punto_inicial[0]-2, -2), max(punto_inicial[0]+2, 2), 50)\n",
    "        y = np.linspace(min(punto_inicial[1]-2, -2), max(punto_inicial[1]+2, 2), 50)\n",
    "        X, Y = np.meshgrid(x, y)\n",
    "        Z = np.zeros_like(X)\n",
    "        \n",
    "        for i in range(X.shape[0]):\n",
    "            for j in range(X.shape[1]):\n",
    "                Z[i,j] = optimizador.f([X[i,j], Y[i,j]])\n",
    "        \n",
    "        # Gráfico de trayectorias\n",
    "        contour = ax1.contour(X, Y, Z, levels=20, alpha=0.6)\n",
    "        ax1.clabel(contour, inline=True, fontsize=8)\n",
    "        \n",
    "        # Trayectoria GD (usar el mejor learning rate)\n",
    "        mejor_lr = min(resultados['descenso_gradiente'].keys(), \n",
    "                      key=lambda k: resultados['descenso_gradiente'][k]['valor_optimo'])\n",
    "        trayectoria_gd = np.array(resultados['descenso_gradiente'][mejor_lr]['trayectoria'])\n",
    "        ax1.plot(trayectoria_gd[:, 0], trayectoria_gd[:, 1], 'ro-', markersize=3, linewidth=1, label='GD')\n",
    "        \n",
    "        # Puntos inicial y final\n",
    "        ax1.plot(punto_inicial[0], punto_inicial[1], 'go', markersize=8, label='Inicio')\n",
    "        ax1.plot(trayectoria_gd[-1, 0], trayectoria_gd[-1, 1], 'bo', markersize=8, label='Final GD')\n",
    "        \n",
    "        ax1.set_xlabel('x')\n",
    "        ax1.set_ylabel('y')\n",
    "        ax1.set_title(f'Trayectorias de Optimización - Punto {punto_inicial}')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Gráfico de convergencia\n",
    "        ax2.semilogy(resultados['descenso_gradiente'][mejor_lr]['valores_funcion'], 'r-', label='GD')\n",
    "        ax2.set_xlabel('Iteración')\n",
    "        ax2.set_ylabel('f(x,y)')\n",
    "        ax2.set_title('Convergencia del Algoritmo')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Visualizar trayectorias para algunos puntos\n",
    "    print(\"\\nVisualizando trayectorias para algunos puntos...\")\n",
    "    puntos_a_visualizar = min(3, len(resultados_completos['experimentos']))\n",
    "    for i in range(puntos_a_visualizar):\n",
    "        exp = resultados_completos['experimentos'][i]\n",
    "        punto = exp['punto_inicial']\n",
    "        print(f\"Punto {i+1}: {punto}\")\n",
    "        visualizar_trayectorias(optimizador, punto, exp)\n",
    "\n",
    "    # Celda 12: Exportar análisis resumido\n",
    "    resumen_analisis = {\n",
    "        'archivo_configuracion': archivo_puntos,\n",
    "        'archivo_resultados': archivo_resultados,\n",
    "        'fecha_analisis': time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        'mejor_algoritmo_global': 'BFGS' if df['bfgs_valor'].mean() < df['gd_valor'].mean() and df['bfgs_valor'].mean() < df['nm_valor'].mean() else 'GD' if df['gd_valor'].mean() < df['nm_valor'].mean() else 'NM',\n",
    "        'valor_minimo_global': min(df['gd_valor'].min(), df['bfgs_valor'].min(), df['nm_valor'].min()),\n",
    "        'eficiencia_promedio': {\n",
    "            'GD_iteraciones': df['gd_iteraciones'].mean(),\n",
    "            'BFGS_iteraciones': df['bfgs_iteraciones'].mean(),\n",
    "            'NM_iteraciones': df['nm_iteraciones'].mean()\n",
    "        },\n",
    "        'precision_promedio': {\n",
    "            'GD_gradiente': df['gd_gradiente'].mean(),\n",
    "            'BFGS_gradiente': df['bfgs_gradiente'].mean()\n",
    "        },\n",
    "        'resumen_por_punto': df.to_dict('records')\n",
    "    }\n",
    "\n",
    "    archivo_resumen = f\"analisis_resumen_{time.strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    with open(archivo_resumen, 'w') as f:\n",
    "        json.dump(resumen_analisis, f, indent=2)\n",
    "\n",
    "    print(f\"\\nAnálisis resumido guardado en: {archivo_resumen}\")\n",
    "    print(\"\\nRESUMEN FINAL:\")\n",
    "    print(f\"Mejor algoritmo global: {resumen_analisis['mejor_algoritmo_global']}\")\n",
    "    print(f\"Valor mínimo encontrado: {resumen_analisis['valor_minimo_global']:.2e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
